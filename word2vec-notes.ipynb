{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vector Representations of Words\n",
    "\n",
    "텐서플로우 튜토리얼을 공부하면서 해석 및 요약 정리한 내용입니다.\n",
    "\n",
    "작성일 : Unknown <br>\n",
    "수정일 : March 6, 2019\n",
    " - 제목 변경, 작업 개요 추가, 작성일, 수정일 추가\n",
    "\n",
    "\n",
    "# Motivation: Why Learn Word Embeddings\n",
    "\n",
    "* NLP 에서는 전통적으로 단어를 이산원자기호(discrete atomic symbol)로 다룸\n",
    "* 이런 기호는 임의적이고 개별 기호간에 관계가 있는 것이 아니면 유용한 정보를 제공하지 않음\n",
    "  * 예를 들어 '고양이'라는 단어를 학습하면서 알게된 정보의 아주 일부만 '강아지'라는 단어를 처리하면서 활용할 수 있음을 의미함\n",
    "* 단어를 고유하고 성긴 데이터로 표현하는 것은 모델 훈련에 더 많은 데이터가 필요함을 의미\n",
    "* 벡터 표현은 이러한 단점을 극복할 수 있음\n",
    "* 단어를 연속적인 벡터 공간에 표현하면 유사한 의미의 단어가 근접한 점에 표현 됨\n",
    "* NLP에서 단어를 벡터 공간에 표현하는 방법은 긴 역사를 갖고 있지만 대부분은 Distributional Hypothesis에 기반함\n",
    "  * 같은 맥락에 표현한 단어는 의미적인 뜻을 공유함을 나타냄\n",
    "* 이러한 원리를 활용하는 방법은 크게 두 종류로 구분 함\n",
    "  * count-based method(e.g. Latent Semantic Analysis)\n",
    "  * predictive method(e.g. neural probabilistic language models)\n",
    "* Word2Vec 은 원문에서 단어 임베딩을 학습하는 계산 효율적인 예측 모델임\n",
    "\n",
    "* CBOW와 skip-gram 모델이 있음\n",
    "\n",
    "# Scaling up with Noise-Contrastive Training\n",
    "\n",
    "* 신경 확률론 언어 모델은 전통적으로 maximum likelihood를 이용\n",
    "* 주어진 단어에 대해 대상 단어의 확률을 최대화하기 위해\n",
    "* softmax 함수를 사용\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "P(w_{t}|h) &= softmax(score(w_{t}, h)) \\\\\n",
    "&= \\frac{\\exp({score(w_{t}, h)})} {\\sum_{Word w^{'} in Vocab}\\exp({score(w^{'}, h)})}\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "* $score(w_t,h)$는 h에서 $w_t$의 적합성을 계산\n",
    "* 훈련 데이터에 대해 log-likelihood를 최대화 시키는 파라메터를 찾음(훈련 과정)\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "J_{ML} &= \\log (P(w_t|h)) \\\\\n",
    "&= score(w_t, h) - \\log ((\\sum_{Word w^{'} in Vocab}\\exp ({score(w^{'}, h)})))\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "* 적합하게 정규화된 언어 확률 모델을 제공\n",
    "* 매 훈련 단계에서 사전에 있는 모든 단어에 대해 현재 문맥 단어에 대한 확률점수를 계산하고 정규화해야 하기 때문에 많은 계산량이 필요함\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "# Summary\n",
    "\n",
    "# Reference\n",
    "* [Tensorflow Tutorial](https://www.tensorflow.org/tutorials/representation/word2vec)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
